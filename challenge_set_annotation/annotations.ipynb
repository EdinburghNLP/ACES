{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b96d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the phenomena: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Loading the test dataset...\n",
      "INFO:logger:Test dataset loaded.\n",
      "INFO:logger:Path /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/manual_annotations/annotated_checkpoint_test.txt already exists. Loading..\n",
      "INFO:logger:Creating new stats.txt file at /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/ACES_private/challenge_set_annotation/stats.txt\n",
      "INFO:logger:READY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import json, copy, os, sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('logger')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "from annotation_utilities import *\n",
    "\n",
    "# this is the list of phenomena and which option they need to be annotated with:\n",
    "phenomena = {\n",
    "    'addition':'add-omit',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-causal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-contrast':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-frequent':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-infrequent':'diff_flexible',\n",
    "    'anaphoric_group_it-they:deletion':'annotate_word',\n",
    "    'anaphoric_group_it-they:substitution':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_they:deletion':'annotate_word',\n",
    "    'anaphoric_intra_they:substitution':'annotate_word',\n",
    "    'anaphoric_singular_they:deletion':'annotate_word',\n",
    "    'anaphoric_singular_they:substitution':'annotate_word',\n",
    "    'antonym-replacement':'REF_flexible',\n",
    "    'commonsense-only-ref-ambiguous':'diff_flexible',\n",
    "    'commonsense-src-and-ref-ambiguous':'diff_flexible',\n",
    "    'copy-source':'whole_sentence',\n",
    "    'coreference-based-on-commonsense':'mixed_flexible',\n",
    "    'do-not-translate':'whole_sentence',\n",
    "    'hallucination-date-time':'date',\n",
    "    'hallucination-named-entity-level-1':'diff_flexible',\n",
    "    'hallucination-named-entity-level-2':'REF_flexible',\n",
    "    'hallucination-named-entity-level-3':'REF_flexible',\n",
    "    'hallucination-number-level-1':'diff_flexible',\n",
    "    'hallucination-number-level-2':'REF_flexible',\n",
    "    'hallucination-number-level-3':'REF_flexible',\n",
    "    'hallucination-real-data-vs-ref-word':'diff_flexible',\n",
    "    'hallucination-real-data-vs-synonym':'diff_flexible',\n",
    "    'hallucination-unit-conversion-amount-matches-ref':'units',\n",
    "    'hallucination-unit-conversion-unit-matches-ref':'units',\n",
    "    'hypernym-replacement':'REF_flexible',\n",
    "    'hyponym-replacement':'REF_flexible',\n",
    "    'lexical-overlap':'?',\n",
    "    'modal_verb:deletion':'add-omit',\n",
    "    'modal_verb:substitution':'diff_flexible',\n",
    "    'nonsense':'REF_flexible',\n",
    "    'omission':'add-omit',\n",
    "    'ordering-mismatch':'swap',\n",
    "    'overly-literal-vs-correct-idiom':'diff_flexible',\n",
    "    'overly-literal-vs-explanation':'diff_flexible',\n",
    "    'overly-literal-vs-ref-word':'diff_flexible',\n",
    "    'overly-literal-vs-synonym':'diff_flexible',\n",
    "    'pleonastic_it:deletion':'annotate_word',\n",
    "    'pleonastic_it:substitution':'annotate_word',\n",
    "    'punctuation:deletion_all':'add-omit',\n",
    "    'punctuation:deletion_commas':'add-omit',\n",
    "    'punctuation:deletion_quotes':'add-omit',\n",
    "    'punctuation:statement-to-question':'add-omit',\n",
    "    'real-world-knowledge-entailment':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-distractor':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-hyponym':'diff_flexible',\n",
    "    'real-world-knowledge-synonym-vs-antonym':'diff_flexible',\n",
    "    'similar-language-high':'whole_sentence',\n",
    "    'similar-language-low':'whole_sentence',\n",
    "    'untranslated-vs-ref-word':'diff_flexible',   # here add-omit can be used for getting character level replacements too\n",
    "    'untranslated-vs-synonym':'diff_flexible',\n",
    "    'xnli-addition-contradiction':'?',\n",
    "    'xnli-addition-neutral':'?',\n",
    "    'xnli-omission-contradiction':'?',\n",
    "    'xnli-omission-neutral':'?'\n",
    "}\n",
    "\n",
    "folder = os.getcwd()\n",
    "manual_annotations = os.path.join(folder, 'manual_annotations')\n",
    "if not os.path.exists(manual_annotations):\n",
    "    os.mkdir(manual_annotations)\n",
    "    \n",
    "phenomena_tobe_processed = input(\"enter the phenomena: \") \n",
    "if phenomena_tobe_processed == 'test':\n",
    "    # load the subset.json\n",
    "    dataset_path = os.path.join(manual_annotations, 'subset.json')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the test dataset...')\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        samples = json.load(f)\n",
    "    logger.info('Test dataset loaded.')\n",
    "elif phenomena_tobe_processed not in phenomena.keys():\n",
    "    logger.error(\"The phenomena should be one of these: {}\".format(sys.argv[1], phenomena.keys()))\n",
    "    exit()\n",
    "else:\n",
    "    dataset_path = os.path.join(folder, '../../dataset')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the dataset...')\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    logger.info('Dataset loaded.')\n",
    "    samples = dict()\n",
    "    for idx, sample in enumerate(dataset['train']):\n",
    "        if sample['phenomena'] in phenomena_tobe_processed:\n",
    "            samples[idx] = sample\n",
    "        \n",
    "checkpoint = os.path.join(folder, 'manual_annotations/annotated_checkpoint_{}.txt'.format(phenomena_tobe_processed))\n",
    "if os.path.exists(checkpoint):\n",
    "    logger.info('Path {} already exists. Loading..'.format(checkpoint))\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "    annotations = {int(k):v for k,v in annotations.items()}\n",
    "else:\n",
    "    annotations = dict()\n",
    "    \n",
    "# calculate statistics about the annotations:\n",
    "# for every mode, calculate no. of skipped, no. of unsure and ids, and no. of done.\n",
    "stats_template = {\n",
    "            'total':0,\n",
    "            'success':0,\n",
    "            'too_long':[],\n",
    "            'no_change':[],\n",
    "            'error':[],\n",
    "            'other':[]  \n",
    "        }\n",
    "stats_path = os.path.join(folder, 'ACES_private/challenge_set_annotation/stats.txt')\n",
    "if os.path.exists(stats_path):\n",
    "    logger.info('Path {} already exists. Loading..'.format(stats_path))\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    # we want to overwrite the statistics for the new phenomena\n",
    "    for p in phenomena_tobe_processed:\n",
    "        stats[p] = copy.deepcopy(stats_template)\n",
    "else:\n",
    "    logger.info('Creating new stats.txt file at {}'.format(stats_path))\n",
    "    stats = {}\n",
    "    for key in phenomena.keys():\n",
    "        stats[key] = copy.deepcopy(stats_template)\n",
    "    stats['test'] = copy.deepcopy(stats_template)\n",
    "    \n",
    "            \n",
    "logger.info(\"READY\")\n",
    "\n",
    "# the UI (?) part of the annotation in general (ask if they want to accept the annotation, call manual_annotation if no)\n",
    "def manual_annotation_io(idx):\n",
    "    sample = samples[idx]\n",
    "    if idx in annotations:\n",
    "        change = annotations[idx]['annotation']   # now it's normalized annotation.\n",
    "        if len(change) == 1 and len(change[0][\"in_good\"]['token_index']) == len(change[0][\"in_bad\"]['token_index']):\n",
    "            return 0\n",
    "    if phenomena[sample[\"phenomena\"]] in ['?', 'mixed_flexible']:\n",
    "        print(\"-----> For this sample we can compare the Incorrect translation with either Reference or Good translation.\")\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['REF_flexible']:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Reference.\")\n",
    "    else:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Good translation.\\n\")\n",
    "    if idx in annotations:\n",
    "        print(\"\\nID: \", idx)\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        print('Suggested annotation:')\n",
    "        print(annotations[idx]['annotation'], '\\n')\n",
    "        inp = input('To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:')\n",
    "        if inp == \"skip\":\n",
    "            annotations.pop(idx)\n",
    "            return 1  # this means, we are skipping, so should delete this annotation and then continue with the next.\n",
    "        if inp == \"exit\":\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        res = manual_annotation(idx, inp)\n",
    "        if res == -1:\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        if res == 1:\n",
    "            # skipping\n",
    "            annotations.pop(idx)\n",
    "            return 1\n",
    "    else:\n",
    "        print(\"No automatic translations for this sample.\")\n",
    "        res = manual_annotation(idx)\n",
    "        if res == -1:\n",
    "            return -1\n",
    "\n",
    "# the UI (?) part of the manual annotation\n",
    "def manual_annotation(idx, inp=\".\"):\n",
    "    while inp != \"\":\n",
    "        sample = samples[idx]\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        inp = input(\"Enter the incorrect translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        bad = inp\n",
    "        if bad == \"exit\":\n",
    "            return -1\n",
    "        if bad == \"skip\":\n",
    "            return 1\n",
    "        inp = input(\"Enter the correct/reference translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        good = inp\n",
    "        if good == \"exit\":\n",
    "            return -1\n",
    "        if good == \"skip\":\n",
    "            return 1\n",
    "        change = calculate_change(good, bad, sample)\n",
    "        print(\"Annotation: \", change)\n",
    "        inp = input(\"\\n To accept it press enter or to annotate again enter any other string: \")\n",
    "        if inp == \"\":\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = \"manual annotation\"\n",
    "            annotations[idx] = sample\n",
    "    return annotations[idx]\n",
    "\n",
    "# given a manually annotated sample (where there are <> in incorrect and good/reference sentences)\n",
    "# calculate the character spans in the original sentences and return the change in our annotation format\n",
    "def calculate_change(good, bad, sample):  \n",
    "    bad_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    change = []\n",
    "    for i, c in enumerate(bad):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = bad_id\n",
    "                start_annotate = i\n",
    "                bad_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":None, \n",
    "                    \"in_bad\":{'token_index':None, \n",
    "                    'character_span':(start,bad_id), \n",
    "                               'token':bad[start_annotate+1:i]}})\n",
    "                bad_id -= 1\n",
    "                span = False\n",
    "        bad_id += 1\n",
    "    good_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    for i, c in enumerate(good):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = good_id\n",
    "                start_annotate = i\n",
    "                good_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":{'token_index':None, \n",
    "                    'character_span':(start,good_id), \n",
    "                               'token':good[start_annotate+1:i]}, \n",
    "                    \"in_bad\":None})\n",
    "                good_id -= 1\n",
    "                span = False\n",
    "        good_id += 1\n",
    "    return change\n",
    "\n",
    "# process given sample, annotate or do manual annotation (only in the annotations.ipynb, in process_dataset.py only automatic annotation)\n",
    "def process_sample(idx, sample, manual=False, detokenize=False):\n",
    "    if phenomena[sample[\"phenomena\"]] == 'mixed_flexible':\n",
    "        good_og = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'REF_flexible':\n",
    "        good_og = sample[\"reference\"]\n",
    "    else:\n",
    "        good_og = sample[\"good-translation\"]\n",
    "    bad_og = sample[\"incorrect-translation\"]\n",
    "    # if detokenize we just annotate the detokenized sentences, then map the character span back to the original sentence\n",
    "    # in the standardize_annotation function in annotation_utilities.py\n",
    "    if detokenize:\n",
    "        try:\n",
    "            good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            maps = (good_map, bad_map)\n",
    "        except:\n",
    "            good, bad = good_og, bad_og\n",
    "            maps = None\n",
    "    else:\n",
    "        good, bad = good_og, bad_og\n",
    "        maps = None # the standardize_annotation function will understand that it does not need to revert detokenization \n",
    "        # if maps parameter is None.\n",
    "    originals = (good_og, bad_og)\n",
    "    \n",
    "    if phenomena[sample[\"phenomena\"]] == 'add-omit':\n",
    "        try:\n",
    "            change = diff_char_level(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in char level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'annotate_word':\n",
    "        try:\n",
    "            change = annotate_word(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in word level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['diff_flexible', 'REF_flexible', 'mixed_flexible']:\n",
    "        g, g_spans = tokenize(good)\n",
    "        b, b_spans = tokenize(bad)\n",
    "\n",
    "        # special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "        if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "            if len(g) == len(b):   # if there are multiple one word replacements\n",
    "                change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "            if len(g) != len(b) or len(change) == 0:\n",
    "                try:\n",
    "                    change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "                    if len(change) == 0 and good != bad:\n",
    "                        change = diff_char_level(good, bad) \n",
    "                except:\n",
    "                    logger.warning('error in id {}'.format(idx))\n",
    "                    stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "                logger.warning('check this - too long: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        else:\n",
    "            try:\n",
    "                change = diff_char_level(good, bad) \n",
    "                if len(change) == 0 and good != bad:\n",
    "                    logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                    stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "                elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 30) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 30)):\n",
    "                    logger.warning('check this - too long: %s' %idx)\n",
    "                    stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "                else:\n",
    "                    stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                    change = standardize_annotation(change, good, bad, maps, originals)\n",
    "                sample['annotation'] = change\n",
    "                sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "                annotations[idx] = sample\n",
    "            except: \n",
    "                logger.warning('error in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'units':\n",
    "        try:\n",
    "            g, b, change = annotate_units(good,bad)\n",
    "            if len(change) == 0 and g != b:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {},\\nr: {}'.format(idx, g, b))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) > 1:\n",
    "                logger.warning('Multiple changes in {} id {}'.format(sample[\"phenomena\"], idx))\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'swap':\n",
    "        try:\n",
    "            change = annotate_swap_word_lvl(good,bad)\n",
    "            if len(change) < 2 and good != bad:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {}'.format(idx, good, bad))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif change[0]['in_good'] != None and change[1]['in_good'] != None and change[0]['in_good'] == change[1]['in_good']:\n",
    "                logger.warning('check this: %s - swapped words are the same!' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            elif (change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50):\n",
    "                logger.warning('check this: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'date':\n",
    "        try:\n",
    "            change = diff_dates(good,bad)\n",
    "            stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "            change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "    elif phenomena[sample['phenomena']] == 'whole_sentence':\n",
    "        change = whole_sentence(good, bad)\n",
    "        stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "        change = standardize_annotation(change, good, bad, maps, originals)\n",
    "        sample['annotation'] = change\n",
    "        sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "        annotations[idx] = sample\n",
    "    if manual:\n",
    "        res = manual_annotation_io(idx)\n",
    "        if res == 1:  # SKIPPING\n",
    "            return 1 \n",
    "        # if exit, first save a new annotations file to save progress and then exit\n",
    "        if res == -1:\n",
    "            with open(checkpoint, \"w+\") as f:\n",
    "                json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON\n",
    "            return -1\n",
    "    return 1  # 1 for success\n",
    "        \n",
    "def process_phenomena(samples, manual=False, detokenize=False):\n",
    "    for idx,sample in tqdm(samples.items()):\n",
    "        # here don't worry about stats - it will be probably completely wrong\n",
    "        if idx not in annotations.keys():\n",
    "            stats[sample[\"phenomena\"]][\"total\"] += 1\n",
    "            \n",
    "            # check if it was annotated before\n",
    "            res = check_seen_before(sample, annotations)\n",
    "            if res != None:\n",
    "                sample['annotation'] = res[0]\n",
    "                sample['method'] = res[1]\n",
    "                annotations[idx] = sample\n",
    "            else:\n",
    "                try:\n",
    "                    res = process_sample(idx, sample, manual, detokenize)\n",
    "                except:\n",
    "                    logger.error(idx)\n",
    "                if res == -1:\n",
    "                    return -1\n",
    "    # save all annotations after finished\n",
    "    with open(checkpoint, \"w+\") as f:\n",
    "        json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a0f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ ----------\n",
      "absl-py                  1.4.0\n",
      "aiohttp                  3.8.4\n",
      "aiosignal                1.3.1\n",
      "asttokens                2.2.1\n",
      "async-timeout            4.0.2\n",
      "attrs                    22.2.0\n",
      "backcall                 0.2.0\n",
      "certifi                  2022.12.7\n",
      "charset-normalizer       3.1.0\n",
      "click                    8.1.3\n",
      "cmake                    3.26.1\n",
      "comm                     0.1.3\n",
      "datasets                 2.11.0\n",
      "debugpy                  1.6.6\n",
      "decorator                5.1.1\n",
      "dill                     0.3.6\n",
      "docopt                   0.6.2\n",
      "executing                1.2.0\n",
      "fastjsonschema           2.16.3\n",
      "filelock                 3.10.7\n",
      "frozenlist               1.3.3\n",
      "fsspec                   2023.3.0\n",
      "huggingface-hub          0.13.3\n",
      "idna                     3.4\n",
      "inflect                  6.0.4\n",
      "iniconfig                2.0.0\n",
      "ipykernel                6.22.0\n",
      "ipython                  8.12.0\n",
      "ipywidgets               8.0.6\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "joblib                   1.2.0\n",
      "jsonschema               4.17.3\n",
      "jupyter_client           8.1.0\n",
      "jupyter_core             5.3.0\n",
      "jupyterlab-widgets       3.0.7\n",
      "lit                      16.0.0\n",
      "MarkupSafe               2.1.2\n",
      "matplotlib-inline        0.1.6\n",
      "mpmath                   1.3.0\n",
      "mt-metrics-eval          0.0.1\n",
      "multidict                6.0.4\n",
      "multiprocess             0.70.14\n",
      "nbformat                 5.8.0\n",
      "nest-asyncio             1.5.6\n",
      "networkx                 3.0\n",
      "num2words                0.5.12\n",
      "numpy                    1.24.2\n",
      "nvidia-cublas-cu11       11.10.3.66\n",
      "nvidia-cuda-cupti-cu11   11.7.101\n",
      "nvidia-cuda-nvrtc-cu11   11.7.99\n",
      "nvidia-cuda-runtime-cu11 11.7.99\n",
      "nvidia-cudnn-cu11        8.5.0.96\n",
      "nvidia-cufft-cu11        10.9.0.58\n",
      "nvidia-curand-cu11       10.2.10.91\n",
      "nvidia-cusolver-cu11     11.4.0.1\n",
      "nvidia-cusparse-cu11     11.7.4.91\n",
      "nvidia-nccl-cu11         2.14.3\n",
      "nvidia-nvtx-cu11         11.7.91\n",
      "packaging                23.0\n",
      "pandas                   1.5.3\n",
      "parso                    0.8.3\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "pip                      23.1.2\n",
      "platformdirs             3.2.0\n",
      "plotly                   5.14.1\n",
      "pluggy                   1.0.0\n",
      "prompt-toolkit           3.0.38\n",
      "psutil                   5.9.4\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "pyarrow                  11.0.0\n",
      "pydantic                 1.10.7\n",
      "Pygments                 2.14.0\n",
      "pyrsistent               0.19.3\n",
      "pytest                   7.3.1\n",
      "python-dateutil          2.8.2\n",
      "pytz                     2023.3\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.0.2\n",
      "quantulum3               0.8.1\n",
      "regex                    2023.5.5\n",
      "requests                 2.28.2\n",
      "responses                0.18.0\n",
      "sacremoses               0.0.53\n",
      "scikit-learn             1.2.2\n",
      "scipy                    1.10.1\n",
      "setuptools               67.7.2\n",
      "six                      1.16.0\n",
      "stack-data               0.6.2\n",
      "sympy                    1.11.1\n",
      "tenacity                 8.2.2\n",
      "threadpoolctl            3.1.0\n",
      "torch                    2.0.0\n",
      "tornado                  6.2\n",
      "tqdm                     4.65.0\n",
      "traitlets                5.9.0\n",
      "triton                   2.0.0\n",
      "typing_extensions        4.5.0\n",
      "urllib3                  1.26.15\n",
      "wcwidth                  0.2.6\n",
      "wheel                    0.40.0\n",
      "widgetsnbextension       4.0.7\n",
      "xxhash                   3.2.0\n",
      "yarl                     1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80f02d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to start from the beginning - not from a checkpoint (you will lose the prev checkpoint tho)\n",
    "annotations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c1d437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/100 [00:00<?, ?it/s]WARNING:logger:check this - too long: 35814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> For this sample we compare the Incorrect translation with the Good translation.\n",
      "\n",
      "\n",
      "ID:  35814\n",
      "Source sentence:  Moreover, top judge Evangelos Kalousis is imprisoned as he found guilty of corruption and degenerate behaviour.\n",
      "Reference:  याउपर, सर्वोच्च न्यायाधीश एवनजेलॉस कलौसिस यांना अटक झाली कारण ते भ्रष्टाचार आणि भ्रष्ट वर्तनासाठी दोषी आढळले.\n",
      "Good Translation:  शिवाय, सर्वोच्च न्यायाधीश इव्हान्जेलोस कालोसिस यांना भ्रष्टाचार आणि अधोगती वर्तनासाठी दोषी आढळल्याने तुरुंगात टाकले आहे.\n",
      "Incorrect Translation:  तसेच, उच्च न्यायाधीश Evangelos Kalousis जबाबदारी आणि भ्रष्टाचार म्हणून दोषी पाहिले म्हणून जेल करण्यात आले आहे.\n",
      "Suggested annotation:\n",
      "[{'in_good': {'token_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 'character_span': (0, 115), 'token': 'शिवाय, सर्वोच्च न्यायाधीश इव्हान्जेलोस कालोसिस यांना भ्रष्टाचार आणि अधोगती वर्तनासाठी दोषी आढळल्याने तुरुंगात टाकले'}, 'in_bad': {'token_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'character_span': (0, 105), 'token': 'तसेच, उच्च न्यायाधीश Evangelos Kalousis जबाबदारी आणि भ्रष्टाचार म्हणून दोषी पाहिले म्हणून जेल करण्यात आले'}}] \n",
      "\n",
      "To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████▊                                            | 35/100 [00:04<00:08,  7.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE ACTUAL PART\n",
    "logger.setLevel(logging.INFO)\n",
    "process_phenomena(samples, manual=True, detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420150d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this cell there are extra stuff - no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1bce7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run normalization over the already annotated samples in coreference-based-on-commonsense\n",
    "# No need to do that for manual annotation!\n",
    "for idx in annotations:\n",
    "    sample = annotations[idx]\n",
    "    good, _, _ = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    bad = sample['incorrect-translation']\n",
    "    change = sample['annotation']\n",
    "    try:\n",
    "        sample['annotation'] = standardize_annotation(change, good, bad)\n",
    "    except:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5871f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint, \"w+\") as f:\n",
    "    json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b899069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the number of samples in any phenomena\n",
    "# No need to do that for manual annotation!\n",
    "count = 0\n",
    "for sample in dataset[\"train\"]:\n",
    "    if sample[\"phenomena\"] in [\"lexical-overlap\", 'xnli-omission-neutral', 'xnli-omission-contradiction', 'xnli-addition-neutral', 'xnli-addition-contradiction']:\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
