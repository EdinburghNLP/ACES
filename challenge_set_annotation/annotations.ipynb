{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b96d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the phenomena: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Loading the test dataset...\n",
      "INFO:logger:Test dataset loaded.\n",
      "INFO:logger:Path /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/manual_annotations/annotated_checkpoint_test.txt already exists. Loading..\n",
      "INFO:logger:Creating new stats.txt file at /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/ACES_private/challenge_set_annotation/stats.txt\n",
      "INFO:logger:READY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import json, copy, os, sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('logger')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "from annotation_utilities import *\n",
    "\n",
    "# this is the list of phenomena and which option they need to be annotated with:\n",
    "phenomena = {\n",
    "    'addition':'add-omit',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-causal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-contrast':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-frequent':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-infrequent':'diff_flexible',\n",
    "    'anaphoric_group_it-they:deletion':'annotate_word',\n",
    "    'anaphoric_group_it-they:substitution':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_they:deletion':'annotate_word',\n",
    "    'anaphoric_intra_they:substitution':'annotate_word',\n",
    "    'anaphoric_singular_they:deletion':'annotate_word',\n",
    "    'anaphoric_singular_they:substitution':'annotate_word',\n",
    "    'antonym-replacement':'REF_flexible',\n",
    "    'commonsense-only-ref-ambiguous':'diff_flexible',\n",
    "    'commonsense-src-and-ref-ambiguous':'diff_flexible',\n",
    "    'copy-source':'whole_sentence',\n",
    "    'coreference-based-on-commonsense':'mixed_flexible',\n",
    "    'do-not-translate':'whole_sentence',\n",
    "    'hallucination-date-time':'date',\n",
    "    'hallucination-named-entity-level-1':'diff_flexible',\n",
    "    'hallucination-named-entity-level-2':'REF_flexible',\n",
    "    'hallucination-named-entity-level-3':'REF_flexible',\n",
    "    'hallucination-number-level-1':'diff_flexible',\n",
    "    'hallucination-number-level-2':'REF_flexible',\n",
    "    'hallucination-number-level-3':'REF_flexible',\n",
    "    'hallucination-real-data-vs-ref-word':'diff_flexible',\n",
    "    'hallucination-real-data-vs-synonym':'diff_flexible',\n",
    "    'hallucination-unit-conversion-amount-matches-ref':'units',\n",
    "    'hallucination-unit-conversion-unit-matches-ref':'units',\n",
    "    'hypernym-replacement':'REF_flexible',\n",
    "    'hyponym-replacement':'REF_flexible',\n",
    "    'lexical-overlap':'?',\n",
    "    'modal_verb:deletion':'add-omit',\n",
    "    'modal_verb:substitution':'diff_flexible',\n",
    "    'nonsense':'REF_flexible',\n",
    "    'omission':'add-omit',\n",
    "    'ordering-mismatch':'swap',\n",
    "    'overly-literal-vs-correct-idiom':'diff_flexible',\n",
    "    'overly-literal-vs-explanation':'diff_flexible',\n",
    "    'overly-literal-vs-ref-word':'diff_flexible',\n",
    "    'overly-literal-vs-synonym':'diff_flexible',\n",
    "    'pleonastic_it:deletion':'annotate_word',\n",
    "    'pleonastic_it:substitution':'annotate_word',\n",
    "    'punctuation:deletion_all':'add-omit',\n",
    "    'punctuation:deletion_commas':'add-omit',\n",
    "    'punctuation:deletion_quotes':'add-omit',\n",
    "    'punctuation:statement-to-question':'add-omit',\n",
    "    'real-world-knowledge-entailment':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-distractor':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-hyponym':'diff_flexible',\n",
    "    'real-world-knowledge-synonym-vs-antonym':'diff_flexible',\n",
    "    'similar-language-high':'whole_sentence',\n",
    "    'similar-language-low':'whole_sentence',\n",
    "    'untranslated-vs-ref-word':'diff_flexible',   # here add-omit can be used for getting character level replacements too\n",
    "    'untranslated-vs-synonym':'diff_flexible',\n",
    "    'xnli-addition-contradiction':'?',\n",
    "    'xnli-addition-neutral':'?',\n",
    "    'xnli-omission-contradiction':'?',\n",
    "    'xnli-omission-neutral':'?'\n",
    "}\n",
    "\n",
    "folder = os.getcwd()\n",
    "manual_annotations = os.path.join(folder, 'manual_annotations')\n",
    "if not os.path.exists(manual_annotations):\n",
    "    os.mkdir(manual_annotations)\n",
    "    \n",
    "phenomena_tobe_processed = input(\"enter the phenomena: \") \n",
    "if phenomena_tobe_processed == 'test':\n",
    "    # load the subset.json\n",
    "    dataset_path = os.path.join(manual_annotations, 'subset.json')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the test dataset...')\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        samples = json.load(f)\n",
    "    logger.info('Test dataset loaded.')\n",
    "elif phenomena_tobe_processed not in phenomena.keys():\n",
    "    logger.error(\"The phenomena should be one of these: {}\".format(sys.argv[1], phenomena.keys()))\n",
    "    exit()\n",
    "else:\n",
    "    dataset_path = os.path.join(folder, '../../dataset')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the dataset...')\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    logger.info('Dataset loaded.')\n",
    "    samples = dict()\n",
    "    for idx, sample in enumerate(dataset['train']):\n",
    "        if sample['phenomena'] in phenomena_tobe_processed:\n",
    "            samples[idx] = sample\n",
    "        \n",
    "checkpoint = os.path.join(folder, 'manual_annotations/annotated_checkpoint_{}.txt'.format(phenomena_tobe_processed))\n",
    "if os.path.exists(checkpoint):\n",
    "    logger.info('Path {} already exists. Loading..'.format(checkpoint))\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "    annotations = {int(k):v for k,v in annotations.items()}\n",
    "else:\n",
    "    annotations = dict()\n",
    "    \n",
    "# calculate statistics about the annotations:\n",
    "# for every mode, calculate no. of skipped, no. of unsure and ids, and no. of done.\n",
    "stats_template = {\n",
    "            'total':0,\n",
    "            'success':0,\n",
    "            'too_long':[],\n",
    "            'no_change':[],\n",
    "            'error':[],\n",
    "            'other':[]  \n",
    "        }\n",
    "stats_path = os.path.join(folder, 'ACES_private/challenge_set_annotation/stats.txt')\n",
    "if os.path.exists(stats_path):\n",
    "    logger.info('Path {} already exists. Loading..'.format(stats_path))\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    # we want to overwrite the statistics for the new phenomena\n",
    "    for p in phenomena_tobe_processed:\n",
    "        stats[p] = copy.deepcopy(stats_template)\n",
    "else:\n",
    "    logger.info('Creating new stats.txt file at {}'.format(stats_path))\n",
    "    stats = {}\n",
    "    for key in phenomena.keys():\n",
    "        stats[key] = copy.deepcopy(stats_template)\n",
    "    stats['test'] = copy.deepcopy(stats_template)\n",
    "    \n",
    "            \n",
    "logger.info(\"READY\")\n",
    "\n",
    "# the UI (?) part of the annotation in general (ask if they want to accept the annotation, call manual_annotation if no)\n",
    "def manual_annotation_io(idx):\n",
    "    sample = samples[idx]\n",
    "    if idx in annotations:\n",
    "        change = annotations[idx]['annotation']   # now it's normalized annotation.\n",
    "        if len(change) == 1 and len(change[0][\"in_good\"]['token_index']) == len(change[0][\"in_bad\"]['token_index']):\n",
    "            return 0\n",
    "    if phenomena[sample[\"phenomena\"]] in ['?', 'mixed_flexible']:\n",
    "        print(\"-----> For this sample we can compare the Incorrect translation with either Reference or Good translation.\")\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['REF_flexible']:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Reference.\")\n",
    "    else:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Good translation.\\n\")\n",
    "    if idx in annotations:\n",
    "        print(\"\\nID: \", idx)\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        print('Suggested annotation:')\n",
    "        print(annotations[idx]['annotation'], '\\n')\n",
    "        inp = input('To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:')\n",
    "        if inp == \"skip\":\n",
    "            annotations.pop(idx)\n",
    "            return 1  # this means, we are skipping, so should delete this annotation and then continue with the next.\n",
    "        if inp == \"exit\":\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        res = manual_annotation(idx, inp)\n",
    "        if res == -1:\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        if res == 1:\n",
    "            # skipping\n",
    "            annotations.pop(idx)\n",
    "            return 1\n",
    "    else:\n",
    "        print(\"No automatic translations for this sample.\")\n",
    "        res = manual_annotation(idx)\n",
    "        if res == -1:\n",
    "            return -1\n",
    "\n",
    "# the UI (?) part of the manual annotation\n",
    "def manual_annotation(idx, inp=\".\"):\n",
    "    while inp != \"\":\n",
    "        sample = samples[idx]\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        inp = input(\"Enter the incorrect translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        bad = inp\n",
    "        if bad == \"exit\":\n",
    "            return -1\n",
    "        if bad == \"skip\":\n",
    "            return 1\n",
    "        inp = input(\"Enter the correct/reference translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        good = inp\n",
    "        if good == \"exit\":\n",
    "            return -1\n",
    "        if good == \"skip\":\n",
    "            return 1\n",
    "        change = calculate_change(good, bad, sample)\n",
    "        print(\"Annotation: \", change)\n",
    "        inp = input(\"\\n To accept it press enter or to annotate again enter any other string: \")\n",
    "        if inp == \"\":\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = \"manual annotation\"\n",
    "            annotations[idx] = sample\n",
    "    return annotations[idx]\n",
    "\n",
    "# given a manually annotated sample (where there are <> in incorrect and good/reference sentences)\n",
    "# calculate the character spans in the original sentences and return the change in our annotation format\n",
    "def calculate_change(good, bad, sample):  \n",
    "    bad_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    change = []\n",
    "    for i, c in enumerate(bad):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = bad_id\n",
    "                start_annotate = i\n",
    "                bad_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":None, \n",
    "                    \"in_bad\":{'token_index':None, \n",
    "                    'character_span':(start,bad_id), \n",
    "                               'token':bad[start_annotate+1:i]}})\n",
    "                bad_id -= 1\n",
    "                span = False\n",
    "        bad_id += 1\n",
    "    good_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    for i, c in enumerate(good):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = good_id\n",
    "                start_annotate = i\n",
    "                good_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":{'token_index':None, \n",
    "                    'character_span':(start,good_id), \n",
    "                               'token':good[start_annotate+1:i]}, \n",
    "                    \"in_bad\":None})\n",
    "                good_id -= 1\n",
    "                span = False\n",
    "        good_id += 1\n",
    "    return change\n",
    "\n",
    "# process given sample, annotate or do manual annotation (only in the annotations.ipynb, in process_dataset.py only automatic annotation)\n",
    "def process_sample(idx, sample, manual=False, detokenize=False):\n",
    "    if phenomena[sample[\"phenomena\"]] == 'mixed_flexible':\n",
    "        good_og = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'REF_flexible':\n",
    "        good_og = sample[\"reference\"]\n",
    "    else:\n",
    "        good_og = sample[\"good-translation\"]\n",
    "    bad_og = sample[\"incorrect-translation\"]\n",
    "    # if detokenize we just annotate the detokenized sentences, then map the character span back to the original sentence\n",
    "    # in the standardize_annotation function in annotation_utilities.py\n",
    "    if detokenize:\n",
    "        try:\n",
    "            good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            maps = (good_map, bad_map)\n",
    "        except:\n",
    "            good, bad = good_og, bad_og\n",
    "            maps = None\n",
    "    else:\n",
    "        good, bad = good_og, bad_og\n",
    "        maps = None # the standardize_annotation function will understand that it does not need to revert detokenization \n",
    "        # if maps parameter is None.\n",
    "    originals = (good_og, bad_og)\n",
    "    \n",
    "    if phenomena[sample[\"phenomena\"]] == 'add-omit':\n",
    "        try:\n",
    "            change = diff_char_level(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in char level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'annotate_word':\n",
    "        try:\n",
    "            change = annotate_word(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in word level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['diff_flexible', 'REF_flexible', 'mixed_flexible']:\n",
    "        g, g_spans = tokenize(good)\n",
    "        b, b_spans = tokenize(bad)\n",
    "\n",
    "        # special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "        if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "            if len(g) == len(b):   # if there are multiple one word replacements\n",
    "                change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "            if len(g) != len(b) or len(change) == 0:\n",
    "                try:\n",
    "                    change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "                    if len(change) == 0 and good != bad:\n",
    "                        change = diff_char_level(good, bad) \n",
    "                except:\n",
    "                    logger.warning('error in id {}'.format(idx))\n",
    "                    stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "                logger.warning('check this - too long: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        else:\n",
    "            try:\n",
    "                change = diff_char_level(good, bad) \n",
    "                if len(change) == 0 and good != bad:\n",
    "                    logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                    stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "                elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 30) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 30)):\n",
    "                    logger.warning('check this - too long: %s' %idx)\n",
    "                    stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "                else:\n",
    "                    stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                    change = standardize_annotation(change, good, bad, maps, originals)\n",
    "                sample['annotation'] = change\n",
    "                sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "                annotations[idx] = sample\n",
    "            except: \n",
    "                logger.warning('error in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'units':\n",
    "        try:\n",
    "            g, b, change = annotate_units(good,bad)\n",
    "            if len(change) == 0 and g != b:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {},\\nr: {}'.format(idx, g, b))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) > 1:\n",
    "                logger.warning('Multiple changes in {} id {}'.format(sample[\"phenomena\"], idx))\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'swap':\n",
    "        try:\n",
    "            change = annotate_swap_word_lvl(good,bad)\n",
    "            if len(change) < 2 and good != bad:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {}'.format(idx, good, bad))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif change[0]['in_good'] != None and change[1]['in_good'] != None and change[0]['in_good'] == change[1]['in_good']:\n",
    "                logger.warning('check this: %s - swapped words are the same!' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            elif (change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50):\n",
    "                logger.warning('check this: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'date':\n",
    "        try:\n",
    "            change = diff_dates(good,bad)\n",
    "            stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "            change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "    elif phenomena[sample['phenomena']] == 'whole_sentence':\n",
    "        change = whole_sentence(good, bad)\n",
    "        stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "        change = standardize_annotation(change, good, bad, maps, originals)\n",
    "        sample['annotation'] = change\n",
    "        sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "        annotations[idx] = sample\n",
    "    if manual:\n",
    "        res = manual_annotation_io(idx)\n",
    "        if res == 1:  # SKIPPING\n",
    "            return 1 \n",
    "        # if exit, first save a new annotations file to save progress and then exit\n",
    "        if res == -1:\n",
    "            with open(checkpoint, \"w+\") as f:\n",
    "                json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON\n",
    "            return -1\n",
    "    return 1  # 1 for success\n",
    "        \n",
    "def process_phenomena(samples, manual=False, detokenize=False):\n",
    "    for idx,sample in tqdm(samples.items()):\n",
    "        # here don't worry about stats - it will be probably completely wrong\n",
    "        if idx not in annotations.keys():\n",
    "            stats[sample[\"phenomena\"]][\"total\"] += 1\n",
    "            \n",
    "            # check if it was annotated before\n",
    "            res = check_seen_before(sample, annotations)\n",
    "            if res != None:\n",
    "                sample['annotation'] = res[0]\n",
    "                sample['method'] = res[1]\n",
    "                annotations[idx] = sample\n",
    "            else:\n",
    "                try:\n",
    "                    res = process_sample(idx, sample, manual, detokenize)\n",
    "                except:\n",
    "                    logger.error(idx)\n",
    "                if res == -1:\n",
    "                    return -1\n",
    "    # save all annotations after finished\n",
    "    with open(checkpoint, \"w+\") as f:\n",
    "        json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80f02d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to start from the beginning - not from a checkpoint (you will lose the prev checkpoint tho)\n",
    "# annotations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1d437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                   | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> For this sample we can compare the Incorrect translation with either Reference or Good translation.\n",
      "\n",
      "ID:  1100\n",
      "Source sentence:  Susan made the decision of getting in an airplane instead of train to go to Miami because it travels fast.\n",
      "Reference:  Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que l' avion voyage vite.\n",
      "Good Translation:  Susan a pris la décision de monter dans un avion au lieu du train pour aller à Miami parce qu'il voyage rapidement.\n",
      "Incorrect Translation:  Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que le train voyage vite.\n",
      "Suggested annotation:\n",
      "[{'in_good': {'token_index': [21], 'character_span': (99, 106), 'token': \"l'avion\"}, 'in_bad': {'token_index': [21, 22], 'character_span': (99, 107), 'token': 'le train'}}] \n",
      "\n",
      "To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:a\n",
      "Source sentence:  Susan made the decision of getting in an airplane instead of train to go to Miami because it travels fast.\n",
      "Reference:  Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que l' avion voyage vite.\n",
      "Good Translation:  Susan a pris la décision de monter dans un avion au lieu du train pour aller à Miami parce qu'il voyage rapidement.\n",
      "Incorrect Translation:  Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que le train voyage vite.\n",
      "Enter the incorrect translation with the < and > to show the error spans (exit to stop, skip to skip): \n",
      "exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▍                                                                                                                   | 6/100 [00:25<06:46,  4.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE ACTUAL PART\n",
    "logger.setLevel(logging.INFO)\n",
    "process_phenomena(samples, manual=True, detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884ec205",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[\"1100\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a8c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Susan made the decision of getting in an airplane instead of train to go to Miami because it travels fast.',\n",
       " 'good-translation': \"Susan a pris la décision de monter dans un avion au lieu du train pour aller à Miami parce qu'il voyage rapidement.\",\n",
       " 'incorrect-translation': 'Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que le train voyage vite.',\n",
       " 'reference': \"Susan a pris la décision de monter dans un avion au lieu du train pour se rendre à Miami parce que l' avion voyage vite.\",\n",
       " 'phenomena': 'coreference-based-on-commonsense',\n",
       " 'langpair': 'en-fr'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf252c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 76,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 120}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c9bda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'in_good': {'token_index': [21],\n",
       "   'character_span': (99, 106),\n",
       "   'token': \"l'avion\"},\n",
       "  'in_bad': {'token_index': [21, 22],\n",
       "   'character_span': (99, 107),\n",
       "   'token': 'le train'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10cbed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'in_good': {'token_index': [21],\n",
       "   'character_span': (99, 106),\n",
       "   'token': \"l'avion\"},\n",
       "  'in_bad': {'token_index': [21, 22],\n",
       "   'character_span': (99, 107),\n",
       "   'token': 'le train'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce85199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1100\n",
    "\n",
    "good_og = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "bad_og = sample[\"incorrect-translation\"]\n",
    "\n",
    "good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "maps = (good_map, bad_map)\n",
    "\n",
    "originals = (good_og, bad_og)\n",
    "g, g_spans = tokenize(good)\n",
    "b, b_spans = tokenize(bad)\n",
    "\n",
    "# special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "    if len(g) == len(b):   # if there are multiple one word replacements\n",
    "        change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "    if len(g) != len(b) or len(change) == 0:\n",
    "        try:\n",
    "            change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "            if len(change) == 0 and good != bad:\n",
    "                change = diff_char_level(good, bad) \n",
    "        except:\n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "    if len(change) == 0:\n",
    "        logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "        stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "    elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "        logger.warning('check this - too long: %s' %idx)\n",
    "        stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "    else:\n",
    "        stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3442f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_utilities import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20718fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'in_good': {'token_index': [21],\n",
       "   'character_span': (99, 106),\n",
       "   'token': \"l'avion\"},\n",
       "  'in_bad': {'token_index': [21, 22],\n",
       "   'character_span': (99, 107),\n",
       "   'token': 'le train'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df03cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'in_good': {'token_index': [21],\n",
       "   'character_span': (99, 107),\n",
       "   'token': \"l' avion\"},\n",
       "  'in_bad': {'token_index': [21],\n",
       "   'character_span': (99, 107),\n",
       "   'token': 'le train'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c6218e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'change_new' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstandardize_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginals\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mstandardize_annotation\u001b[0;34m(change, good, bad, maps, original)\u001b[0m\n\u001b[1;32m      3\u001b[0m good_mapping, bad_mapping \u001b[38;5;241m=\u001b[39m maps[\u001b[38;5;241m0\u001b[39m], maps[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m good_og, bad_og \u001b[38;5;241m=\u001b[39m original[\u001b[38;5;241m0\u001b[39m], original[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchange_new\u001b[49m:\n\u001b[1;32m      6\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      7\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'change_new' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "standardize_annotation(change, good, bad, maps, originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce502fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def standardize_annotation(change, good, bad, maps=None, original=None):\n",
    "    change_tmp = copy.deepcopy(change)\n",
    "    if maps != None:\n",
    "        good_mapping, bad_mapping = maps[0], maps[1]\n",
    "        good_og, bad_og = original[0], original[1]\n",
    "        for c in change_tmp:\n",
    "            c[\"in_good\"]['character_span'] = (good_mapping[c[\"in_good\"]['character_span'][0]], good_mapping[c[\"in_good\"]['character_span'][1]])\n",
    "            c[\"in_bad\"]['character_span'] = (bad_mapping[c[\"in_bad\"]['character_span'][0]], bad_mapping[c[\"in_bad\"]['character_span'][1]])\n",
    "            c[\"in_good\"]['token'] = good_og[c[\"in_good\"]['character_span'][0]:c[\"in_good\"]['character_span'][1]]\n",
    "            c[\"in_bad\"]['token'] = bad_og[c[\"in_bad\"]['character_span'][0]:c[\"in_bad\"]['character_span'][1]]\n",
    "        \n",
    "    skip = False\n",
    "    for c in change:\n",
    "        if (c['in_good'] != None and (c['in_good']['token_index'] == None or (type(c['in_good']['token_index'])==list and len(c['in_good']['token_index']) > 1)))\\\n",
    "        or (c['in_bad'] != None and (c['in_bad']['token_index'] == None or (type(c['in_bad']['token_index'])==list and len(c['in_bad']['token_index']) > 1)))\\\n",
    "        or c['in_good'] == None or c['in_bad'] == None:\n",
    "            skip = True\n",
    "            logger.debug(\"first check\")\n",
    "            break\n",
    "    if skip:   # if skipping then change all the integer token indices to lists\n",
    "        for c in change:\n",
    "            if c['in_good'] != None and c['in_good']['token_index'] != None and type(c['in_good']['token_index']) != list:\n",
    "                c['in_good']['token_index'] = [c['in_good']['token_index']]\n",
    "            if c['in_bad'] != None and c['in_bad']['token_index'] != None and type(c['in_bad']['token_index']) != list:\n",
    "                c['in_bad']['token_index'] = [c['in_bad']['token_index']]\n",
    "        print(\"here\")\n",
    "        return change\n",
    "    good_tokens = []\n",
    "    bad_tokens = []\n",
    "    good_span = ()   # char span\n",
    "    bad_span = ()\n",
    "    change_new = []\n",
    "    for c in change:\n",
    "        g = c['in_good']\n",
    "        b = c['in_bad']\n",
    "        if type(g['token_index']) == list:\n",
    "            g['token_index'] = g['token_index'][0]\n",
    "        if type(b['token_index']) == list:\n",
    "            b['token_index'] = b['token_index'][0]\n",
    "        if len(good_tokens) == 0 and len(bad_tokens) == 0:\n",
    "            good_tokens.append(g['token_index'])\n",
    "            bad_tokens.append(b['token_index'])\n",
    "            good_span = g['character_span']\n",
    "            bad_span = b['character_span']\n",
    "        elif g['token_index'] == good_tokens[-1] + 1 and b['token_index'] == bad_tokens[-1] + 1:\n",
    "            good_tokens.append(g['token_index'])\n",
    "            bad_tokens.append(b['token_index'])\n",
    "            good_span = (good_span[0], g['character_span'][1])\n",
    "            bad_span = (bad_span[0], b['character_span'][1])\n",
    "        else:\n",
    "            change_new.append({'in_good': {'token_index': good_tokens,\n",
    "                        'character_span': good_span,\n",
    "                        'token': good[good_span[0]:good_span[1]]}, \n",
    "                    'in_bad': {'token_index': bad_tokens,\n",
    "                        'character_span': bad_span,\n",
    "                        'token': bad[bad_span[0]:bad_span[1]]}})\n",
    "            good_tokens = [g['token_index']]\n",
    "            bad_tokens = [b['token_index']]\n",
    "            good_span = g['character_span']\n",
    "            bad_span = b['character_span']\n",
    "\n",
    "    change_new.append({'in_good': {'token_index': good_tokens,\n",
    "                        'character_span': good_span,\n",
    "                        'token': good[good_span[0]:good_span[1]]}, \n",
    "                    'in_bad': {'token_index': bad_tokens,\n",
    "                        'character_span': bad_span,\n",
    "                        'token': bad[bad_span[0]:bad_span[1]]}})\n",
    "    \n",
    "    \n",
    "    return change_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this cell there are extra stuff - no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1bce7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run normalization over the already annotated samples in coreference-based-on-commonsense\n",
    "# No need to do that for manual annotation!\n",
    "for idx in annotations:\n",
    "    sample = annotations[idx]\n",
    "    good, _, _ = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    bad = sample['incorrect-translation']\n",
    "    change = sample['annotation']\n",
    "    try:\n",
    "        sample['annotation'] = standardize_annotation(change, good, bad)\n",
    "    except:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5871f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint, \"w+\") as f:\n",
    "    json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b899069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the number of samples in any phenomena\n",
    "# No need to do that for manual annotation!\n",
    "count = 0\n",
    "for sample in dataset[\"train\"]:\n",
    "    if sample[\"phenomena\"] in [\"lexical-overlap\", 'xnli-omission-neutral', 'xnli-omission-contradiction', 'xnli-addition-neutral', 'xnli-addition-contradiction']:\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
