{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b96d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the phenomena: overly-literal-vs-ref-word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Loading the dataset...\n",
      "INFO:logger:Dataset loaded.\n",
      "INFO:logger:READY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "import json, copy, os, sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('logger')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "from annotation_utilities import *\n",
    "\n",
    "# this is the list of phenomena and which option they need to be annotated with:\n",
    "phenomena = {\n",
    "    'addition':'add-omit',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-causal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-contrast':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-frequent':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-infrequent':'diff_flexible',\n",
    "    'anaphoric_group_it-they:deletion':'annotate_word',\n",
    "    'anaphoric_group_it-they:substitution':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_they:deletion':'annotate_word',\n",
    "    'anaphoric_intra_they:substitution':'annotate_word',\n",
    "    'anaphoric_singular_they:deletion':'annotate_word',\n",
    "    'anaphoric_singular_they:substitution':'annotate_word',\n",
    "    'antonym-replacement':'REF_flexible',\n",
    "    'commonsense-only-ref-ambiguous':'diff_flexible',\n",
    "    'commonsense-src-and-ref-ambiguous':'diff_flexible',\n",
    "    'copy-source':'whole_sentence',\n",
    "    'coreference-based-on-commonsense':'mixed_flexible',\n",
    "    'do-not-translate':'whole_sentence',\n",
    "    'hallucination-date-time':'date',\n",
    "    'hallucination-named-entity-level-1':'diff_flexible',\n",
    "    'hallucination-named-entity-level-2':'REF_flexible',\n",
    "    'hallucination-named-entity-level-3':'REF_flexible',\n",
    "    'hallucination-number-level-1':'diff_flexible',\n",
    "    'hallucination-number-level-2':'REF_flexible',\n",
    "    'hallucination-number-level-3':'REF_flexible',\n",
    "    'hallucination-real-data-vs-ref-word':'diff_flexible',\n",
    "    'hallucination-real-data-vs-synonym':'diff_flexible',\n",
    "    'hallucination-unit-conversion-amount-matches-ref':'units',\n",
    "    'hallucination-unit-conversion-unit-matches-ref':'units',\n",
    "    'hypernym-replacement':'REF_flexible',\n",
    "    'hyponym-replacement':'REF_flexible',\n",
    "    'lexical-overlap':'?',\n",
    "    'modal_verb:deletion':'add-omit',\n",
    "    'modal_verb:substitution':'diff_flexible',\n",
    "    'nonsense':'REF_flexible',\n",
    "    'omission':'add-omit',\n",
    "    'ordering-mismatch':'swap',\n",
    "    'overly-literal-vs-correct-idiom':'diff_flexible',\n",
    "    'overly-literal-vs-explanation':'diff_flexible',\n",
    "    'overly-literal-vs-ref-word':'diff_flexible',\n",
    "    'overly-literal-vs-synonym':'diff_flexible',\n",
    "    'pleonastic_it:deletion':'annotate_word',\n",
    "    'pleonastic_it:substitution':'annotate_word',\n",
    "    'punctuation:deletion_all':'add-omit',\n",
    "    'punctuation:deletion_commas':'add-omit',\n",
    "    'punctuation:deletion_quotes':'add-omit',\n",
    "    'punctuation:statement-to-question':'add-omit',\n",
    "    'real-world-knowledge-entailment':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-distractor':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-hyponym':'diff_flexible',\n",
    "    'real-world-knowledge-synonym-vs-antonym':'diff_flexible',\n",
    "    'similar-language-high':'whole_sentence',\n",
    "    'similar-language-low':'whole_sentence',\n",
    "    'untranslated-vs-ref-word':'diff_flexible',   # here add-omit can be used for getting character level replacements too\n",
    "    'untranslated-vs-synonym':'diff_flexible',\n",
    "    'xnli-addition-contradiction':'?',\n",
    "    'xnli-addition-neutral':'?',\n",
    "    'xnli-omission-contradiction':'?',\n",
    "    'xnli-omission-neutral':'?'\n",
    "}\n",
    "\n",
    "folder = os.getcwd()\n",
    "manual_annotations = os.path.join(folder, 'manual_annotations')\n",
    "if not os.path.exists(manual_annotations):\n",
    "    os.mkdir(manual_annotations)\n",
    "    \n",
    "phenomena_tobe_processed = input(\"enter the phenomena: \") \n",
    "if phenomena_tobe_processed == 'test':\n",
    "    # load the subset.json\n",
    "    dataset_path = os.path.join(manual_annotations, 'subset.json')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the test dataset...')\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        samples = json.load(f)\n",
    "    logger.info('Test dataset loaded.')\n",
    "elif phenomena_tobe_processed == 'dataset':\n",
    "    dataset_path = os.path.join(folder, '../../dataset')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the dataset...')\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    logger.info('Dataset loaded.')\n",
    "elif phenomena_tobe_processed in phenomena.keys():\n",
    "    dataset_path = os.path.join(folder, '../../dataset')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error('No dataset path: %s' %(dataset_path))\n",
    "        exit()\n",
    "    logger.info('Loading the dataset...')\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    logger.info('Dataset loaded.')\n",
    "    samples = dict()\n",
    "    for idx, sample in enumerate(dataset['train']):\n",
    "        if sample['phenomena'] in phenomena_tobe_processed:\n",
    "            samples[idx] = sample        \n",
    "else:\n",
    "    logger.error(\"The phenomena should be one of these: {}\".format(sys.argv[1], phenomena.keys()))\n",
    "        \n",
    "checkpoint = os.path.join(folder, 'manual_annotations/annotated_checkpoint_{}.txt'.format(phenomena_tobe_processed))\n",
    "if os.path.exists(checkpoint):\n",
    "    logger.info('Path {} already exists. Loading..'.format(checkpoint))\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "    annotations = {str(k):v for k,v in annotations.items()}\n",
    "else:\n",
    "    annotations = dict()\n",
    "\n",
    "logger.info(\"READY\")\n",
    "\n",
    "def display_annotation(idx, sample):\n",
    "    m1 = \"<mark>\"\n",
    "    m2 = \"</mark>\"\n",
    "    m_len = len(m1) + len(m2)\n",
    "    n_spans = 0\n",
    "    change = sample[\"annotation\"]\n",
    "    bad_new = sample[\"incorrect-translation\"]\n",
    "    for c in change:\n",
    "        if c[\"in_bad\"] != None:\n",
    "            span = c[\"in_bad\"][\"character_span\"]\n",
    "            indices = (span[0]+n_spans*m_len, span[1]+n_spans*m_len)\n",
    "            bad_new = bad_new[:indices[0]] + m1 + bad_new[indices[0]:indices[1]] + m2 + bad_new[indices[1]:]\n",
    "            n_spans += 1\n",
    "    html = '''\n",
    "    <style>\n",
    "    mark {{\n",
    "        background-color: #4CAF50;\n",
    "        color: black;\n",
    "      }}\n",
    "    </style>\n",
    "    <body>\n",
    "    <h3>{}</h3>\n",
    "    <p>Source: {}</p>\n",
    "    <p>Reference: {}</p>\n",
    "    <p>Good translation: {}</p>\n",
    "    <p>Incorrect: {}</p>\n",
    "    <p>Phenomenon: {}</p>\n",
    "    </body>\n",
    "    '''.format(idx, sample['source'], sample['reference'], sample['good-translation'], bad_new, sample['phenomena'])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def display_sample(idx, sample):\n",
    "    html = '''\n",
    "    <style>\n",
    "    mark {{\n",
    "        background-color: #4CAF50;\n",
    "        color: black;\n",
    "      }}\n",
    "    </style>\n",
    "    <body>\n",
    "    <h3>{}</h3>\n",
    "    <p>Source: {}</p>\n",
    "    <p>Reference: {}</p>\n",
    "    <p>Good translation: {}</p>\n",
    "    <p>Incorrect: {}</p>\n",
    "    <p>Phenomenon: {}</p>\n",
    "    </body>\n",
    "    '''.format(idx, sample['source'], sample['reference'], sample['good-translation'], sample['incorrect-translation'], sample['phenomena'])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def display_annotation_old(idx, sample):\n",
    "    print(\"\\nID: \", idx)\n",
    "    print(\"Source sentence: \", sample['source'])\n",
    "    print(\"Reference: \", sample['reference'])\n",
    "    print(\"Good Translation: \", sample['good-translation'])\n",
    "    print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "    print('Suggested annotation:')\n",
    "    print(annotations[idx]['annotation'], '\\n')\n",
    "    \n",
    "# the UI (?) part of the annotation in general (ask if they want to accept the annotation, call manual_annotation if no)\n",
    "def manual_annotation_io(idx):\n",
    "    sample = samples[idx]\n",
    "    if idx in annotations:\n",
    "        change = annotations[idx]['annotation']   # now it's normalized annotation.\n",
    "        if len(change) == 1 and len(change[0][\"in_good\"]['token_index']) == len(change[0][\"in_bad\"]['token_index']):\n",
    "            return 0\n",
    "    if phenomena[sample[\"phenomena\"]] in ['?', 'mixed_flexible']:\n",
    "        print(\"-----> For this sample we can compare the Incorrect translation with either Reference or Good translation.\")\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['REF_flexible']:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Reference.\")\n",
    "    else:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Good translation.\\n\")\n",
    "    if idx in annotations:\n",
    "        display_annotation(idx, sample)\n",
    "        inp = input('To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:')\n",
    "        if inp == \"skip\":\n",
    "            annotations.pop(idx)\n",
    "            return 1  # this means, we are skipping, so should delete this annotation and then continue with the next.\n",
    "        if inp == \"exit\":\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        res = manual_annotation(idx, inp)\n",
    "        if res == -1:\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "        if res == 1:\n",
    "            # skipping\n",
    "            annotations.pop(idx)\n",
    "            return 1\n",
    "    else:\n",
    "        display_sample(idx, sample)\n",
    "        print(\"No automatic translations for this sample.\")\n",
    "        res = manual_annotation(idx)\n",
    "        if res == -1:\n",
    "            return -1\n",
    "\n",
    "# the UI (?) part of the manual annotation\n",
    "def manual_annotation(idx, inp=\".\"):\n",
    "    while inp != \"\":\n",
    "        sample = samples[idx]\n",
    "        inp = input(\"Enter the incorrect translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        bad = inp\n",
    "        if bad == \"exit\":\n",
    "            return -1\n",
    "        if bad == \"skip\":\n",
    "            return 1\n",
    "        inp = input(\"Enter the correct/reference translation with the < and > to show the error spans (exit to stop, skip to skip): \\n\")\n",
    "        good = inp\n",
    "        if good == \"exit\":\n",
    "            return -1\n",
    "        if good == \"skip\":\n",
    "            return 1\n",
    "        \n",
    "        change = calculate_change(good, bad, sample)\n",
    "        tmp = copy.deepcopy(sample)\n",
    "        tmp[\"annotation\"] = change\n",
    "        display_annotation(idx, tmp)\n",
    "        inp = input(\"\\n To accept it press enter or to annotate again enter any other string: \")\n",
    "        if inp == \"\":\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = \"manual annotation\"\n",
    "            annotations[idx] = sample\n",
    "    return annotations[idx]\n",
    "\n",
    "# given a manually annotated sample (where there are <> in incorrect and good/reference sentences)\n",
    "# calculate the character spans in the original sentences and return the change in our annotation format\n",
    "def calculate_change(good, bad, sample):  \n",
    "    bad_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    change = []\n",
    "    for i, c in enumerate(bad):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = bad_id\n",
    "                start_annotate = i\n",
    "                bad_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":None, \n",
    "                    \"in_bad\":{'token_index':None, \n",
    "                    'character_span':(start,bad_id), \n",
    "                               'token':bad[start_annotate+1:i]}})\n",
    "                bad_id -= 1\n",
    "                span = False\n",
    "        bad_id += 1\n",
    "    good_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    for i, c in enumerate(good):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = good_id\n",
    "                start_annotate = i\n",
    "                good_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":{'token_index':None, \n",
    "                    'character_span':(start,good_id), \n",
    "                               'token':good[start_annotate+1:i]}, \n",
    "                    \"in_bad\":None})\n",
    "                good_id -= 1\n",
    "                span = False\n",
    "        good_id += 1\n",
    "    return change\n",
    "\n",
    "# process given sample, annotate or do manual annotation (only in the annotations.ipynb, in process_dataset.py only automatic annotation)\n",
    "def process_sample(idx, sample, manual=False, detokenize=False):\n",
    "    if phenomena[sample[\"phenomena\"]] == 'mixed_flexible':\n",
    "        good_og = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'REF_flexible':\n",
    "        good_og = sample[\"reference\"]\n",
    "    else:\n",
    "        good_og = sample[\"good-translation\"]\n",
    "    bad_og = sample[\"incorrect-translation\"]\n",
    "    # if detokenize we just annotate the detokenized sentences, then map the character span back to the original sentence\n",
    "    # in the standardize_annotation function in annotation_utilities.py\n",
    "    if detokenize:\n",
    "        try:\n",
    "            good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            maps = (good_map, bad_map)\n",
    "        except:\n",
    "            good, bad = good_og, bad_og\n",
    "            maps = None\n",
    "    else:\n",
    "        good, bad = good_og, bad_og\n",
    "        maps = None # the standardize_annotation function will understand that it does not need to revert detokenization \n",
    "        # if maps parameter is None.\n",
    "    originals = (good_og, bad_og)\n",
    "    \n",
    "    if phenomena[sample[\"phenomena\"]] == 'add-omit':\n",
    "        try:\n",
    "            change = diff_char_level(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "            else:\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in char level annotate, id {}'.format(idx))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'annotate_word':\n",
    "        try:\n",
    "            change = annotate_word(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "            else:\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in word level annotate, id {}'.format(idx))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['diff_flexible', 'REF_flexible', 'mixed_flexible']:\n",
    "        g, g_spans = tokenize(good)\n",
    "        b, b_spans = tokenize(bad)\n",
    "\n",
    "        # special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "        if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "            if len(g) == len(b):   # if there are multiple one word replacements\n",
    "                change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "            if len(g) != len(b) or len(change) == 0:\n",
    "                try:\n",
    "                    change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "                    if len(change) == 0 and good != bad:\n",
    "                        change = diff_char_level(good, bad) \n",
    "                except:\n",
    "                    logger.warning('error in id {}'.format(idx))\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "            elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "                logger.warning('check this - too long: %s' %idx)\n",
    "            else:\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        else:\n",
    "            try:\n",
    "                change = diff_char_level(good, bad) \n",
    "                if len(change) == 0 and good != bad:\n",
    "                    logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 30) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 30)):\n",
    "                    logger.warning('check this - too long: %s' %idx)\n",
    "                else:\n",
    "                    change = standardize_annotation(change, good, bad, maps, originals)\n",
    "                sample['annotation'] = change\n",
    "                sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "                annotations[idx] = sample\n",
    "            except: \n",
    "                logger.warning('error in id {}'.format(idx))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'units':\n",
    "        try:\n",
    "            g, b, change = annotate_units(good,bad)\n",
    "            if len(change) == 0 and g != b:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {},\\nr: {}'.format(idx, g, b))\n",
    "            elif len(change) > 1:\n",
    "                logger.warning('Multiple changes in {} id {}'.format(sample[\"phenomena\"], idx))\n",
    "            else:\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'swap':\n",
    "        try:\n",
    "            change = annotate_swap_word_lvl(good,bad)\n",
    "            if len(change) < 2 and good != bad:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {}'.format(idx, good, bad))\n",
    "            elif change[0]['in_good'] != None and change[1]['in_good'] != None and change[0]['in_good'] == change[1]['in_good']:\n",
    "                logger.warning('check this: %s - swapped words are the same!' %idx)\n",
    "            elif (change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50):\n",
    "                logger.warning('check this: %s' %idx)\n",
    "            else:\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'date':\n",
    "        try:\n",
    "            change = diff_dates(good,bad)\n",
    "            change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "    elif phenomena[sample['phenomena']] == 'whole_sentence':\n",
    "        change = whole_sentence(good, bad)\n",
    "        change = standardize_annotation(change, good, bad, maps, originals)\n",
    "        sample['annotation'] = change\n",
    "        sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "        annotations[idx] = sample\n",
    "    if manual:\n",
    "        res = manual_annotation_io(idx)\n",
    "        if res == 1:  # SKIPPING\n",
    "            return 1 \n",
    "        # if exit, first save a new annotations file to save progress and then exit\n",
    "        if res == -1:\n",
    "            with open(checkpoint, \"w+\") as f:\n",
    "                json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON\n",
    "            return -1\n",
    "    return 1  # 1 for success\n",
    "        \n",
    "def process_phenomena(samples, manual=False, detokenize=False):\n",
    "    for idx,sample in tqdm(samples.items()):\n",
    "        if idx not in annotations.keys() and int(idx) not in annotations.keys():            \n",
    "            # check if it was annotated before\n",
    "            res = check_seen_before(sample, annotations)\n",
    "            if res != None:\n",
    "                sample['annotation'] = res[0]\n",
    "                sample['method'] = res[1]\n",
    "                annotations[int(idx)] = sample\n",
    "            else:\n",
    "                try:\n",
    "                    res = process_sample(idx, sample, manual, detokenize)\n",
    "                except:\n",
    "                    logger.error(idx)\n",
    "                if res == -1:\n",
    "                    return -1\n",
    "    # save all annotations after finished\n",
    "    with open(checkpoint, \"w+\") as f:\n",
    "        json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80f02d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to start from the beginning - not from a checkpoint (you will lose the prev checkpoint tho)\n",
    "# annotations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1d437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                      | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> For this sample we compare the Incorrect translation with the Good translation.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    mark {\n",
       "        background-color: #4CAF50;\n",
       "        color: black;\n",
       "      }\n",
       "    </style>\n",
       "    <body>\n",
       "    <h3>19767</h3>\n",
       "    <p>Source: The Icelandic Meteorological Office also reported no earthquake activity in the Hekla area in the past 48 hours.</p>\n",
       "    <p>Reference: Der isländische Wetterdienst hat ebenfalls keine Erdbebenaktivität im Gebiet um die Hekla in den letzten 48 Stunden festgestellt.</p>\n",
       "    <p>Good translation: Der isländische Wetterdienst meldete auch keine Erdbebenaktivität in der Region Hekla in den letzten 48 Stunden.</p>\n",
       "    <p>Incorrect: <mark>Das isländische Meteorologische Büro</mark> meldete auch keine Erdbebenaktivität in der Region Hekla in den letzten 48 Stunden.</p>\n",
       "    <p>Phenomenon: overly-literal-vs-ref-word</p>\n",
       "    </body>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To accept the suggested annotation click on enter. To skip this one enter skip. To exit enter exit and enter anything else to manually annotate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151/151 [00:04<00:00, 36.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# THE ACTUAL PART\n",
    "logger.setLevel(logging.INFO)\n",
    "process_phenomena(samples, manual=True, detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786c4324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this cell there are extra stuff - no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1bce7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run normalization over the already annotated samples in coreference-based-on-commonsense\n",
    "# No need to do that for manual annotation!\n",
    "for idx in annotations:\n",
    "    sample = annotations[idx]\n",
    "    good, _, _ = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    bad = sample['incorrect-translation']\n",
    "    change = sample['annotation']\n",
    "    try:\n",
    "        sample['annotation'] = standardize_annotation(change, good, bad)\n",
    "    except:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5871f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint, \"w+\") as f:\n",
    "    json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fc91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking if all langpairs are correct\n",
    "langpairs = set()\n",
    "for sample in dataset[\"train\"]:\n",
    "    langpairs.add(sample[\"langpair\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f26a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "langpairs_keys = list(langpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the number of samples in any phenomena\n",
    "# No need to do that for manual annotation!\n",
    "basic_lang = ['en', 'de', 'fr', 'mr', 'tr']\n",
    "for p in ['coreference-based-on-commonsense', 'lexical-overlap',\n",
    "         'xnli-addition-contradiction', 'xnli-addition-neutral',\n",
    "         'xnli-omission-contradiction', 'xnli-omission-neutral']:\n",
    "    print(\"**********************\", p)\n",
    "    l_found = dict(zip(langpairs_keys, np.zeros(len(langpairs))))\n",
    "    for sample in dataset[\"train\"]:\n",
    "        src, tgt = sample[\"langpair\"].split('-')\n",
    "        if sample['phenomena'] == p and l_found[sample[\"langpair\"]] == 0 and (src not in basic_lang or tgt not in basic_lang):\n",
    "            print(sample[\"langpair\"], \": \\n\\t\", sample['source'], \"\\n\\t\", sample['good-translation'])\n",
    "            l_found[sample[\"langpair\"]] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
