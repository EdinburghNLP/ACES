{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4b96d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the phenomena: overly-literal-vs-explanation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Loading the dataset...\n",
      "INFO:logger:Dataset loaded.\n",
      "INFO:logger:Creating new annotations.txt file at /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/annotated.txt\n",
      "INFO:logger:Creating new stats.txt file at /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/stats.txt\n",
      "INFO:logger:READY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import json, copy, os, sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('logger')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "from annotation_utilities import *\n",
    "\n",
    "# this is the list of phenomena and which option they need to be annotated with:\n",
    "phenomena = {\n",
    "    'addition':'add-omit',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-causal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-since-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-contrast':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-discourse-connective-while-temporal':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-female-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-anti':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-gender-male-pro':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-frequent':'diff_flexible',\n",
    "    'ambiguous-translation-wrong-sense-infrequent':'diff_flexible',\n",
    "    'anaphoric_group_it-they:deletion':'annotate_word',\n",
    "    'anaphoric_group_it-they:substitution':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_non-subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:deletion':'annotate_word',\n",
    "    'anaphoric_intra_subject_it:substitution':'annotate_word',\n",
    "    'anaphoric_intra_they:deletion':'annotate_word',\n",
    "    'anaphoric_intra_they:substitution':'annotate_word',\n",
    "    'anaphoric_singular_they:deletion':'annotate_word',\n",
    "    'anaphoric_singular_they:substitution':'annotate_word',\n",
    "    'antonym-replacement':'REF_flexible',\n",
    "    'commonsense-only-ref-ambiguous':'diff_flexible',\n",
    "    'commonsense-src-and-ref-ambiguous':'diff_flexible',\n",
    "    'copy-source':'whole_sentence',\n",
    "    'coreference-based-on-commonsense':'mixed_flexible',\n",
    "    'do-not-translate':'whole_sentence',\n",
    "    'hallucination-date-time':'date',\n",
    "    'hallucination-named-entity-level-1':'diff_flexible',\n",
    "    'hallucination-named-entity-level-2':'REF_flexible',\n",
    "    'hallucination-named-entity-level-3':'REF_flexible',\n",
    "    'hallucination-number-level-1':'diff_flexible',\n",
    "    'hallucination-number-level-2':'REF_flexible',\n",
    "    'hallucination-number-level-3':'REF_flexible',\n",
    "    'hallucination-real-data-vs-ref-word':'diff_flexible',\n",
    "    'hallucination-real-data-vs-synonym':'diff_flexible',\n",
    "    'hallucination-unit-conversion-amount-matches-ref':'units',\n",
    "    'hallucination-unit-conversion-unit-matches-ref':'units',\n",
    "    'hypernym-replacement':'REF_flexible',\n",
    "    'hyponym-replacement':'REF_flexible',\n",
    "    'lexical-overlap':'?',\n",
    "    'modal_verb:deletion':'add-omit',\n",
    "    'modal_verb:substitution':'diff_flexible',\n",
    "    'nonsense':'REF_flexible',\n",
    "    'omission':'add-omit',\n",
    "    'ordering-mismatch':'swap',\n",
    "    'overly-literal-vs-correct-idiom':'diff_flexible',\n",
    "    'overly-literal-vs-explanation':'diff_flexible',\n",
    "    'overly-literal-vs-ref-word':'diff_flexible',\n",
    "    'overly-literal-vs-synonym':'diff_flexible',\n",
    "    'pleonastic_it:deletion':'annotate_word',\n",
    "    'pleonastic_it:substitution':'annotate_word',\n",
    "    'punctuation:deletion_all':'add-omit',\n",
    "    'punctuation:deletion_commas':'add-omit',\n",
    "    'punctuation:deletion_quotes':'add-omit',\n",
    "    'punctuation:statement-to-question':'add-omit',\n",
    "    'real-world-knowledge-entailment':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-distractor':'diff_flexible',\n",
    "    'real-world-knowledge-hypernym-vs-hyponym':'diff_flexible',\n",
    "    'real-world-knowledge-synonym-vs-antonym':'diff_flexible',\n",
    "    'similar-language-high':'whole_sentence',\n",
    "    'similar-language-low':'whole_sentence',\n",
    "    'untranslated-vs-ref-word':'diff_flexible',   # here add-omit can be used for getting character level replacements too\n",
    "    'untranslated-vs-synonym':'diff_flexible',\n",
    "    'xnli-addition-contradiction':'?',\n",
    "    'xnli-addition-neutral':'?',\n",
    "    'xnli-omission-contradiction':'?',\n",
    "    'xnli-omission-neutral':'?'\n",
    "}\n",
    "\n",
    "phenomena_tobe_processed = input(\"enter the phenomena: \") \n",
    "if phenomena_tobe_processed not in phenomena.keys():\n",
    "        logger.error(\"The phenomena should be one of these: {}\".format(sys.argv[1], phenomena.keys()))\n",
    "        exit()\n",
    "\n",
    "folder = os.getcwd()\n",
    "dataset_path = os.path.join(folder, '../../dataset')\n",
    "if not os.path.exists(dataset_path):\n",
    "    logger.error('No dataset path: %s' %(dataset_path))\n",
    "    exit()\n",
    "\n",
    "logger.info('Loading the dataset...')\n",
    "dataset = load_from_disk(dataset_path)\n",
    "logger.info('Dataset loaded.')\n",
    "\n",
    "\n",
    "# if there are already some annotations overwrite them and append new ones\n",
    "annotated_dataset_path = os.path.join(folder, 'annotated.txt')\n",
    "if os.path.exists(annotated_dataset_path):\n",
    "    logger.info('Path {} already exists. Loading..'.format(annotated_dataset_path))\n",
    "    with open(annotated_dataset_path, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "    annotations = {int(k):v for k,v in annotations.items()}\n",
    "else:\n",
    "    logger.info('Creating new annotations.txt file at {}'.format(annotated_dataset_path))\n",
    "    annotations = dict()\n",
    "\n",
    "# calculate statistics about the annotations:\n",
    "# for every mode, calculate no. of skipped, no. of unsure and ids, and no. of done.\n",
    "stats_template = {\n",
    "            'total':0,\n",
    "            'success':0,\n",
    "            'too_long':[],\n",
    "            'no_change':[],\n",
    "            'error':[],\n",
    "            'skipped':[],\n",
    "            'other':[]  \n",
    "        }\n",
    "stats_path = os.path.join(folder, 'stats.txt')\n",
    "if os.path.exists(stats_path):\n",
    "    logger.info('Path {} already exists. Loading..'.format(stats_path))\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    # we want to overwrite the statistics for the new phenomena\n",
    "    for p in phenomena_tobe_processed:\n",
    "        stats[p] = copy.deepcopy(stats_template)\n",
    "else:\n",
    "    logger.info('Creating new stats.txt file at {}'.format(stats_path))\n",
    "    stats = {}\n",
    "    for key in phenomena.keys():\n",
    "        stats[key] = copy.deepcopy(stats_template)\n",
    "logger.info(\"READY\")\n",
    "\n",
    "# the UI (?) part of the annotation in general (ask if they want to accept the annotation, call manual_annotation if no)\n",
    "def manual_annotation_io(idx):\n",
    "    sample = dataset['train'][idx]\n",
    "    if idx in annotations:\n",
    "        change = annotations[idx]['annotation']   # now it's normalized annotation.\n",
    "        if len(change) == 1 and len(change[0][\"in_good\"]['token_index']) == len(change[0][\"in_bad\"]['token_index']):\n",
    "            return 0\n",
    "    if phenomena[sample[\"phenomena\"]] in ['?', 'mixed_flexible']:\n",
    "        print(\"-----> For this sample we can compare the Incorrect translation with either Reference or Good translation.\")\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['REF_flexible']:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Reference.\")\n",
    "    else:\n",
    "        print(\"-----> For this sample we compare the Incorrect translation with the Good translation.\\n\")\n",
    "    if idx in annotations:\n",
    "        print(\"\\nID: \", idx)\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        print('Suggested annotation:')\n",
    "        print(annotations[idx]['annotation'], '\\n')\n",
    "        inp = input('To accept the suggested annotation click on enter. To skip this one enter skip. Otherwise enter anything else:')\n",
    "        if inp == \"skip\":\n",
    "            annotations.pop(idx)\n",
    "            return 1  # this means, we are skipping, so should delete this annotation and then continue with the next.\n",
    "        res = manual_annotation(idx, inp)\n",
    "        if res == -1:\n",
    "            # do not add the annotation if you stop at this point\n",
    "            annotations.pop(idx)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"No automatic translations for this sample.\")\n",
    "        res = manual_annotation(idx)\n",
    "        if res == -1:\n",
    "            return -1\n",
    "\n",
    "# the UI (?) part of the manual annotation\n",
    "def manual_annotation(idx, inp=\".\"):\n",
    "    while inp != \"\":\n",
    "        sample = dataset['train'][idx]\n",
    "        print(\"Source sentence: \", sample['source'])\n",
    "        print(\"Reference: \", sample['reference'])\n",
    "        print(\"Good Translation: \", sample['good-translation'])\n",
    "        print(\"Incorrect Translation: \", sample['incorrect-translation'])\n",
    "        inp = input(\"Enter the incorrect translation with the < and > to show the error spans (exit to stop): \\n\")\n",
    "        bad = inp\n",
    "        if bad == \"exit\":\n",
    "            return -1\n",
    "        inp = input(\"Enter the correct/reference translation with the < and > to show the error spans (exit to stop): \\n\")\n",
    "        good = inp\n",
    "        if good == \"exit\":\n",
    "            return -1\n",
    "        change = calculate_change(good, bad, sample)\n",
    "        print(\"Annotation: \", change)\n",
    "        inp = input(\"\\n To accept it press enter or to annotate again enter any other string: \")\n",
    "        if inp == \"\":\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = \"manual annotation\"\n",
    "            annotations[idx] = sample\n",
    "    return annotations[idx]\n",
    "\n",
    "# given a manually annotated sample (where there are <> in incorrect and good/reference sentences)\n",
    "# calculate the character spans in the original sentences and return the change in our annotation format\n",
    "def calculate_change(good, bad, sample):  \n",
    "    bad_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    change = []\n",
    "    for i, c in enumerate(bad):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = bad_id\n",
    "                start_annotate = i\n",
    "                bad_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":None, \n",
    "                    \"in_bad\":{'token_index':None, \n",
    "                    'character_span':(start,bad_id), \n",
    "                               'token':bad[start_annotate+1:i]}})\n",
    "                bad_id -= 1\n",
    "                span = False\n",
    "        bad_id += 1\n",
    "    good_id = 0\n",
    "    span = False # False is when we are not inside a span, True is inside a span\n",
    "    for i, c in enumerate(good):\n",
    "        if c == \"<\":\n",
    "            if span:\n",
    "                logger.error(\"< not closed. Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                start = good_id\n",
    "                start_annotate = i\n",
    "                good_id -= 1\n",
    "                span = True\n",
    "        elif c == \">\":\n",
    "            if not span:\n",
    "                logger.error(\"No opening < Try again.\\n\")\n",
    "                return manual_annotation(\".\", sample)\n",
    "            else:\n",
    "                change.append({\"in_good\":{'token_index':None, \n",
    "                    'character_span':(start,good_id), \n",
    "                               'token':good[start_annotate+1:i]}, \n",
    "                    \"in_bad\":None})\n",
    "                good_id -= 1\n",
    "                span = False\n",
    "        good_id += 1\n",
    "    return change\n",
    "\n",
    "\n",
    "\n",
    "# process given sample, annotate or do manual annotation (only in the annotations.ipynb, in process_dataset.py only automatic annotation)\n",
    "def process_sample(idx, sample, manual=False, detokenize=False):\n",
    "    if phenomena[sample[\"phenomena\"]] == 'mixed_flexible':\n",
    "        good_og = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'REF_flexible':\n",
    "        good_og = sample[\"reference\"]\n",
    "    else:\n",
    "        good_og = sample[\"good-translation\"]\n",
    "    bad_og = sample[\"incorrect-translation\"]\n",
    "    # if detokenize we just annotate the detokenized sentences, then map the character span back to the original sentence\n",
    "    # in the standardize_annotation function in annotation_utilities.py\n",
    "    if detokenize:\n",
    "        try:\n",
    "            good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "            maps = (good_map, bad_map)\n",
    "        except:\n",
    "            good, bad = good_og, bad_og\n",
    "            maps = None\n",
    "    else:\n",
    "        good, bad = good_og, bad_og\n",
    "        maps = None # the standardize_annotation function will understand that it does not need to revert detokenization \n",
    "        # if maps parameter is None.\n",
    "    originals = (good_og, bad_og)\n",
    "    \n",
    "    if phenomena[sample[\"phenomena\"]] == 'add-omit':\n",
    "        try:\n",
    "            change = diff_char_level(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in char level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'annotate_word':\n",
    "        try:\n",
    "            change = annotate_word(good, bad)\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except:\n",
    "            logger.warning('error in word level annotate, id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] in ['diff_flexible', 'REF_flexible', 'mixed_flexible']:\n",
    "        g, g_spans = tokenize(good)\n",
    "        b, b_spans = tokenize(bad)\n",
    "\n",
    "        # special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "        if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "            if len(g) == len(b):   # if there are multiple one word replacements\n",
    "                change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "            if len(g) != len(b) or len(change) == 0:\n",
    "                try:\n",
    "                    change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "                    if len(change) == 0 and good != bad:\n",
    "                        change = diff_char_level(good, bad) \n",
    "                except:\n",
    "                    logger.warning('error in id {}'.format(idx))\n",
    "                    stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "            if len(change) == 0:\n",
    "                logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "                logger.warning('check this - too long: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                print(change, good, bad, maps, originals)\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        else:\n",
    "            try:\n",
    "                change = diff_char_level(good, bad) \n",
    "                if len(change) == 0 and good != bad:\n",
    "                    logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "                    stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "                elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 30) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 30)):\n",
    "                    logger.warning('check this - too long: %s' %idx)\n",
    "                    stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "                else:\n",
    "                    stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                    change = standardize_annotation(change, good, bad, maps, originals)\n",
    "                sample['annotation'] = change\n",
    "                sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "                annotations[idx] = sample\n",
    "            except: \n",
    "                logger.warning('error in id {}'.format(idx))\n",
    "                stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'units':\n",
    "        try:\n",
    "            g, b, change = annotate_units(good,bad)\n",
    "            if len(change) == 0 and g != b:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {},\\nr: {}'.format(idx, g, b))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif len(change) > 1:\n",
    "                logger.warning('Multiple changes in {} id {}'.format(sample[\"phenomena\"], idx))\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample  \n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'swap':\n",
    "        try:\n",
    "            change = annotate_swap_word_lvl(good,bad)\n",
    "            if len(change) < 2 and good != bad:\n",
    "                logger.warning('No change in id {}, \\ng: {}, \\nb: {}'.format(idx, good, bad))\n",
    "                stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "            elif change[0]['in_good'] != None and change[1]['in_good'] != None and change[0]['in_good'] == change[1]['in_good']:\n",
    "                logger.warning('check this: %s - swapped words are the same!' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"other\"].append((idx, sample['langpair']))\n",
    "            elif (change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50):\n",
    "                logger.warning('check this: %s' %idx)\n",
    "                stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "            else:\n",
    "                stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "                change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "\n",
    "    elif phenomena[sample[\"phenomena\"]] == 'date':\n",
    "        try:\n",
    "            change = diff_dates(good,bad)\n",
    "            stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "            change = standardize_annotation(change, good, bad, maps, originals)\n",
    "            sample['annotation'] = change\n",
    "            sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "            annotations[idx] = sample\n",
    "        except: \n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "    elif phenomena[sample['phenomena']] == 'whole_sentence':\n",
    "        change = whole_sentence(good, bad)\n",
    "        stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "        change = standardize_annotation(change, good, bad, maps, originals)\n",
    "        sample['annotation'] = change\n",
    "        sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "        annotations[idx] = sample\n",
    "    if manual:\n",
    "        res = manual_annotation_io(idx)\n",
    "        if res == 1:  # SKIPPING\n",
    "            return 1 \n",
    "        # if exit, first save a new annotations file to save progress and then exit\n",
    "        if res == -1:\n",
    "            with open(checkpoint, \"w+\") as f:\n",
    "                json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON\n",
    "            return -1\n",
    "    return 1  # 1 for success\n",
    "        \n",
    "def process_phenomena(samples, manual=False, detokenize=False):\n",
    "    for idx,sample in tqdm(samples):\n",
    "        # here don't worry about stats - it will be probably completely wrong\n",
    "        if idx not in annotations.keys():\n",
    "            stats[sample[\"phenomena\"]][\"total\"] += 1\n",
    "            \n",
    "            # check if it was annotated before\n",
    "            res = check_seen_before(sample)\n",
    "            if res != None:\n",
    "                sample['annotation'] = res[0]\n",
    "                sample['method'] = res[1]\n",
    "                annotations[idx] = sample\n",
    "            else:\n",
    "                try:\n",
    "                    res = process_sample(idx, sample, manual, detokenize)\n",
    "                except:\n",
    "                    logger.error(idx)\n",
    "                if res == -1:\n",
    "                    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3a13100e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Sir Thomas Dutton (1 August 1421 -- 23 September 1459) was a medieval English knight. He was the son of Sir John Dutton and Margaret Savage',\n",
       " 'good-translation': 'Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage.',\n",
       " 'incorrect-translation': 'Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick',\n",
       " 'reference': 'Sir Thomas Dutton (1. August 1421 – 23. September 1459) war ein mittelalterlicher Ritter aus England. Er war der Sohn von Sir John Dutton und Margaret Savage',\n",
       " 'phenomena': 'hallucination-named-entity-level-1',\n",
       " 'langpair': 'en-de'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset['train'][19151]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e54ba6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'in_good': {'token_index': 21, 'character_span': (128, 136), 'token': 'margaret'}, 'in_bad': {'token_index': 21, 'character_span': (128, 134), 'token': 'stevie'}}, {'in_good': {'token_index': 22, 'character_span': (137, 143), 'token': 'savage'}, 'in_bad': {'token_index': 22, 'character_span': (135, 139), 'token': 'dick'}}] Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage. Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick ({0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 13, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 75, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 85, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 119, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143}, {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 13, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 75, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 85, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 119, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138}) ('Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage.', 'Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "139",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocess_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m19151\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 343\u001b[0m, in \u001b[0;36mprocess_sample\u001b[0;34m(idx, sample, manual, detokenize)\u001b[0m\n\u001b[1;32m    341\u001b[0m     stats[sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphenomena\u001b[39m\u001b[38;5;124m\"\u001b[39m]][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mprint\u001b[39m(change, good, bad, maps, originals)\n\u001b[0;32m--> 343\u001b[0m     change \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m change\n\u001b[1;32m    345\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m phenomena[sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphenomena\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/annotation_utilities.py:416\u001b[0m, in \u001b[0;36mstandardize_annotation\u001b[0;34m(change, good, bad, maps, original)\u001b[0m\n\u001b[1;32m    414\u001b[0m good_og, bad_og \u001b[38;5;241m=\u001b[39m original[\u001b[38;5;241m0\u001b[39m], original[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m change_new:\n\u001b[0;32m--> 416\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    417\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    418\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m good_og[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]:c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 139"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "process_sample(19151, sample, manual=False, detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02b5b903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f12c9a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 13,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 75,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 85,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 119,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138,\n",
       " 139: 139,\n",
       " 140: 140,\n",
       " 141: 141,\n",
       " 142: 142,\n",
       " 143: 143}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f67a2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a03d836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'in_good': {'token_index': 21, 'character_span': (128, 136), 'token': 'margaret'}, 'in_bad': {'token_index': 21, 'character_span': (128, 134), 'token': 'stevie'}}, {'in_good': {'token_index': 22, 'character_span': (137, 143), 'token': 'savage'}, 'in_bad': {'token_index': 22, 'character_span': (135, 139), 'token': 'dick'}}] Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage. Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick ({0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 13, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 75, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 85, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 119, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143}, {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 13, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 75, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 85, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 119, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138}) ('Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage.', 'Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "139",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     stats[sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphenomena\u001b[39m\u001b[38;5;124m\"\u001b[39m]][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(change, good, bad, maps, originals)\n\u001b[0;32m---> 28\u001b[0m     change \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m change\n\u001b[1;32m     30\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m phenomena[sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphenomena\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/annotation_utilities.py:416\u001b[0m, in \u001b[0;36mstandardize_annotation\u001b[0;34m(change, good, bad, maps, original)\u001b[0m\n\u001b[1;32m    414\u001b[0m good_og, bad_og \u001b[38;5;241m=\u001b[39m original[\u001b[38;5;241m0\u001b[39m], original[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m change_new:\n\u001b[0;32m--> 416\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    417\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    418\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m good_og[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]:c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 139"
     ]
    }
   ],
   "source": [
    "good = sample[\"good-translation\"]\n",
    "bad = sample[\"incorrect-translation\"]\n",
    "\n",
    "g, g_spans = tokenize(good)\n",
    "b, b_spans = tokenize(bad)\n",
    "\n",
    "# special treatment to japanese chinese and thailandish because they don't use spaces, so can't be split            \n",
    "if sample['langpair'][-2:] not in ['ja', 'zh', 'th']:      \n",
    "    if len(g) == len(b):   # if there are multiple one word replacements\n",
    "        change = diff(g, g_spans, b, b_spans, phenomena=\"replacement\")\n",
    "    if len(g) != len(b) or len(change) == 0:\n",
    "        try:\n",
    "            change = diff_flexible(good, g, g_spans, bad, b, b_spans)\n",
    "            if len(change) == 0 and good != bad:\n",
    "                change = diff_char_level(good, bad) \n",
    "        except:\n",
    "            logger.warning('error in id {}'.format(idx))\n",
    "            stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))\n",
    "    if len(change) == 0:\n",
    "        logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "        stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "    elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 50) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 50)):\n",
    "        logger.warning('check this - too long: %s' %idx)\n",
    "        stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "    else:\n",
    "        stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "        print(change, good, bad, maps, originals)\n",
    "        change = standardize_annotation(change, good, bad, maps, originals)\n",
    "    sample['annotation'] = change\n",
    "    sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "    annotations[idx] = sample  \n",
    "else:\n",
    "    try:\n",
    "        change = diff_char_level(good, bad) \n",
    "        if len(change) == 0 and good != bad:\n",
    "            logger.warning('No change in id {}'.format(idx,g,b,change))\n",
    "            stats[sample[\"phenomena\"]][\"no_change\"].append((idx, sample['langpair']))\n",
    "        elif len(change) != 0 and ((change[0]['in_good'] != None and len(change[0]['in_good']['token']) > 30) or (change[0]['in_bad'] != None and len(change[0]['in_bad']['token']) > 30)):\n",
    "            logger.warning('check this - too long: %s' %idx)\n",
    "            stats[sample[\"phenomena\"]][\"too_long\"].append((idx, sample['langpair']))\n",
    "        else:\n",
    "            stats[sample[\"phenomena\"]][\"success\"] += 1\n",
    "            change = standardize_annotation(change, good, bad, maps, originals)\n",
    "        sample['annotation'] = change\n",
    "        sample['method'] = phenomena[sample[\"phenomena\"]]\n",
    "        annotations[idx] = sample\n",
    "    except: \n",
    "        logger.warning('error in id {}'.format(idx))\n",
    "        stats[sample[\"phenomena\"]][\"error\"].append((idx, sample['langpair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19382c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'in_good': {'token_index': 21,\n",
       "   'character_span': (128, 136),\n",
       "   'token': 'margaret'},\n",
       "  'in_bad': {'token_index': 21,\n",
       "   'character_span': (128, 134),\n",
       "   'token': 'stevie'}},\n",
       " {'in_good': {'token_index': 22,\n",
       "   'character_span': (137, 143),\n",
       "   'token': 'savage'},\n",
       "  'in_bad': {'token_index': 22,\n",
       "   'character_span': (135, 139),\n",
       "   'token': 'dick'}}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c8c92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_change = [{'in_good': {'token_index': [21, 22],\n",
    "   'character_span': (128, 143),\n",
    "   'token': 'Margaret Savage'},\n",
    "  'in_bad': {'token_index': [21, 22],\n",
    "   'character_span': (128, 139),\n",
    "   'token': 'Stevie Dick'}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "539415cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stevie Dick'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad[128:139]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e2e63667",
   "metadata": {},
   "outputs": [],
   "source": [
    "good, good_map = detokenize_text(good_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "bad, bad_map = detokenize_text(bad_og, lang=sample[\"langpair\"].split('-')[1])\n",
    "maps = (good_map, bad_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "754619b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "139",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m originals \u001b[38;5;241m=\u001b[39m  (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstandardize_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/annotation_utilities.py:416\u001b[0m, in \u001b[0;36mstandardize_annotation\u001b[0;34m(change, good, bad, maps, original)\u001b[0m\n\u001b[1;32m    414\u001b[0m good_og, bad_og \u001b[38;5;241m=\u001b[39m original[\u001b[38;5;241m0\u001b[39m], original[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m change_new:\n\u001b[0;32m--> 416\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    417\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    418\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m good_og[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]:c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 139"
     ]
    }
   ],
   "source": [
    "originals =  ('Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Margaret Savage.', 'Sir Thomas Dutton (1. August 1421-23. September 1459) war ein englischer Ritter im Mittelalter und Sohn von Sir John Dutton und Stevie Dick')\n",
    "standardize_annotation(change, good, bad, maps=maps, original=originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d9aa9924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 13,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 75,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 85,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 119,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67f85f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_new = [{'in_good': {'token_index': [21, 22],\n",
    "   'character_span': (128, 143),\n",
    "   'token': 'Margaret Savage'},\n",
    "  'in_bad': {'token_index': [21, 22],\n",
    "   'character_span': (128, 139),\n",
    "   'token': 'Stevie Dick'}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b964d095",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "139",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m change_new:\n\u001b[1;32m      4\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], good_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (bad_mapping[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], \u001b[43mbad_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min_bad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharacter_span\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m good_og[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]:c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_good\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      7\u001b[0m     c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bad_og[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]:c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter_span\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 139"
     ]
    }
   ],
   "source": [
    "good_mapping, bad_mapping = maps[0], maps[1]\n",
    "good_og, bad_og = originals[0], originals[1]\n",
    "for c in change_new:\n",
    "    c[\"in_good\"]['character_span'] = (good_mapping[c[\"in_good\"]['character_span'][0]], good_mapping[c[\"in_good\"]['character_span'][1]])\n",
    "    c[\"in_bad\"]['character_span'] = (bad_mapping[c[\"in_bad\"]['character_span'][0]], bad_mapping[c[\"in_bad\"]['character_span'][1]])\n",
    "    c[\"in_good\"]['token'] = good_og[c[\"in_good\"]['character_span'][0]:c[\"in_good\"]['character_span'][1]]\n",
    "    c[\"in_bad\"]['token'] = bad_og[c[\"in_bad\"]['character_span'][0]:c[\"in_bad\"]['character_span'][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3804733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Path /mnt/c/Users/user/OneDrive/Masaüstü/work/ACES_private/challenge_set_annotation/manual_annotations/annotated_checkpoint_antonym-replacement.txt already exists. Loading..\n"
     ]
    }
   ],
   "source": [
    "# set up continuing\n",
    "samples = []\n",
    "for idx, sample in enumerate(dataset['train']):\n",
    "    if sample['phenomena'] in phenomena_tobe_processed:\n",
    "        samples.append((idx, sample))\n",
    "        \n",
    "# if manual:\n",
    "if not os.path.exists(os.path.join(folder, 'manual_annotations')):\n",
    "    os.mkdir(os.path.join(folder, 'manual_annotations'))\n",
    "checkpoint = os.path.join(folder, 'manual_annotations/annotated_checkpoint_{}.txt'.format(phenomena_tobe_processed))\n",
    "if os.path.exists(checkpoint):\n",
    "    logger.info('Path {} already exists. Loading..'.format(checkpoint))\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "    annotations = {int(k):v for k,v in annotations.items()}\n",
    "else:\n",
    "    annotations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13b52aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start from the beginning\n",
    "# annotations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8c1d437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 790/790 [00:02<00:00, 325.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# THE ACTUAL PART\n",
    "logger.setLevel(logging.INFO)\n",
    "process_phenomena(samples, manual=True, detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6d798a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run normalization over the already annotated samples in coreference-based-on-commonsense\n",
    "for idx in annotations:\n",
    "    sample = annotations[idx]\n",
    "    good, _, _ = ref_or_good(sample[\"reference\"], sample[\"good-translation\"], sample[\"incorrect-translation\"])\n",
    "    bad = sample['incorrect-translation']\n",
    "    change = sample['annotation']\n",
    "    try:\n",
    "        sample['annotation'] = standardize_annotation(change, good, bad)\n",
    "    except:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b899069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the number of samples in any phenomena\n",
    "count = 0\n",
    "for sample in dataset[\"train\"]:\n",
    "    if sample[\"phenomena\"] in [\"lexical-overlap\", 'xnli-omission-neutral', 'xnli-omission-contradiction', 'xnli-addition-neutral', 'xnli-addition-contradiction']:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f932b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint, \"w+\") as f:\n",
    "    json.dump(annotations, f, indent=2, ensure_ascii=False)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "384d26da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': \"Le chauffeur Alok Nath de la superstar Jackie Shroff n'a pas pu faire son travail et a été remplacé par son fils Deepak (Rahul Roy).\",\n",
       " 'good-translation': 'Alok Nath, the driver of superstar Jackie Shroff, was not able to do his job so that he was replaced by his son Deepak (Rahul Roy).',\n",
       " 'incorrect-translation': \"Superstar Jackie Shroff 's driver Alok Nath was not able to do his job so he was replaced by his daughter Deepak (Rahul Roy).\",\n",
       " 'reference': \"Superstar Jackie Shroff's driver Alok Nath was not able to do his job so he was replaced by his son Deepak (Rahul Roy).\",\n",
       " 'phenomena': 'antonym-replacement',\n",
       " 'langpair': 'fr-en',\n",
       " 'annotation': [{'in_good': {'token_index': [19],\n",
       "    'character_span': (96, 99),\n",
       "    'token': 'son'},\n",
       "   'in_bad': {'token_index': [19],\n",
       "    'character_span': (97, 105),\n",
       "    'token': 'daughter'}}],\n",
       " 'method': 'REF_flexible'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[5299]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
